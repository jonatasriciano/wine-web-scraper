  Project Documentation - Wine Data Collection API

Wine Data Collection API
========================

This project is designed to automatically gather information about wines available in Canada. We will build a system that collects data from various wine websites and presents it in a way that other software can access it easily through an API (a system interface).

To do this, we will create "robots" or "scrapers" that can visit wine websites, gather information about the wines, and send this data to a centralized system. Think of it like having a digital assistant that goes to various online stores, picks up the wine details, and organizes everything for you.

1\. Project Overview
--------------------

We are building a system that can:

*   Collect information about wines from multiple websites.
*   Store that information temporarily or in a database.
*   Provide the collected information to other systems through a web API.

The system will use two key technologies:

*   **Web Scraping**: This is the process of extracting information from websites. It's like sending a robot to a store's website to read the details of each wine available there.
*   **RPA (Robotic Process Automation)**: This allows the robot to automate tasks such as navigating websites, selecting information, and then storing or using that data.

The project consists of two main components:

*   **Web Scrapers (Robots)**: Each website will have a unique scraper that understands how to extract wine data from it.
*   **API (Application Programming Interface)**: This is like a waiter at a restaurant. When you request wine data, the API goes to the kitchen (the scrapers and database), gets the data, and brings it back to you in a clean format.

2\. Project Structure
---------------------

We will organize the project as follows:

*   **Scrapers (Robots)**: These are scripts that fetch data from each website.
*   **API**: The API will provide the collected data. Other software can make requests to the API to get the wine data.
*   **Docker**: This is a tool to package everything together and ensure the system runs consistently on any computer.
*   **Database (Optional)**: We may use a database to temporarily store the wine data before sending it through the API.

### Folder Structure

The project will be organized into different folders and files:

            /project
            ├── bots/                     # Scrapers for each website
            │   ├── site\_a\_scraper.py     # Scraper for Site A
            │   └── site\_b\_scraper.py     # Scraper for Site B
            ├── api/                      # The API that provides the data
            │   ├── app.py                # Main code of the API
            │   └── requirements.txt      # API dependencies
            ├── docker-compose.yml        # Docker configuration file
            └── Dockerfile                # Instructions to create the system container
        

This structure helps to keep everything organized and ensures each part of the system works together smoothly.

3\. Web Scraping (How Data is Collected)
----------------------------------------

### What is Web Scraping?

Web scraping is like sending a robot to a website to automatically gather information. For example, a wine website might list the name, price, region, and type of wine. The robot will visit the site, read these details, and collect them for later use.

### Tools We Use for Scraping

To gather data, we will use two main tools:

*   **BeautifulSoup**: This tool helps extract data from simpler websites that don’t have complex features like dynamic content (JavaScript). It is ideal for static websites that show the wine data directly in the HTML code of the page.
*   **Selenium**: This tool is used for more complex websites where the content is generated by JavaScript. Selenium can interact with websites just like a human does — clicking buttons, scrolling, and waiting for content to appear.

### Scraping a Simple Website with BeautifulSoup

Let’s look at how a scraper for a simple website works using BeautifulSoup:

            import requests
            from bs4 import BeautifulSoup

            # Fetch data from the website
            response = requests.get('https://www.example-wine-site.com')
            
            if response.status\_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')

                # Extract wine details
                wines = \[\]
                for wine\_item in soup.find\_all('div', class\_='wine-item'):
                    wine\_data = {
                        'name': wine\_item.find('h2').text.strip(),
                        'price': wine\_item.find('span', class\_='price').text.strip(),
                        'region': wine\_item.find('span', class\_='region').text.strip(),
                    }
                    wines.append(wine\_data)

                print(wines)
        

In this code, we request the website’s content, then use BeautifulSoup to find the details of each wine, like its name, price, and region. We store this information in a list and print it out.

### Scraping a Dynamic Website with Selenium

For websites that use JavaScript to show wine data, we use Selenium. Here’s an example:

            from selenium import webdriver
            from selenium.webdriver.common.by import By
            import time

            # Set up Selenium to use Chrome
            driver = webdriver.Chrome()

            # Open the website
            driver.get('https://www.dynamic-wine-site.com')

            # Wait for the page to load
            time.sleep(5)

            # Extract wine details
            wines = \[\]
            wine\_elements = driver.find\_elements(By.CLASS\_NAME, 'wine-item')
            for element in wine\_elements:
                name = element.find\_element(By.CLASS\_NAME, 'wine-name').text
                price = element.find\_element(By.CLASS\_NAME, 'wine-price').text
                wines.append({'name': name, 'price': price})

            # Close the browser
            driver.quit()

            print(wines)
        

In this case, Selenium is used to open the website and wait for the content to load. Then it extracts the wine name and price, just like before, but this time it can handle the dynamic content generated by JavaScript.

4\. The API (How Data is Provided to Others)
--------------------------------------------

### What is an API?

Think of an API as a bridge that allows different software systems to talk to each other. It’s like ordering food in a restaurant. You (the software) tell the waiter (the API) what you want (wine data), and the waiter brings it to you from the kitchen (the scraper and the database).

### How the API Works

The API allows other systems to make requests for wine data. Here are the main actions it will handle:

*   **GET /wines**: This request gets all the wine data that has been collected so far.
*   **POST /scrape/{site\_id}**: This request tells the system to start gathering data from a specific wine website.
*   **GET /health**: This checks whether the system is running properly.

### API Example

This is how the API is built in Python using the Flask library. It listens for requests and returns data when asked:

            from flask import Flask, jsonify
            from bots.site\_a\_scraper import scrape\_site\_a

            app = Flask(\_\_name\_\_)

            # Route to list all wines
            @app.route('/wines', methods=\['GET'\])
            def list\_wines():
                wines = scrape\_site\_a()  # Call the scraper to collect data
                return jsonify(wines), 200  # Return the wine data as a JSON response

            if \_\_name\_\_ == '\_\_main\_\_':
                app.run(debug=True)
        

This simple code sets up a route that listens for requests to get wine data. When someone asks for the wine list, the API calls the scraper, collects the data, and sends it back as a response.

5\. Docker (Running Everything Smoothly)
----------------------------------------

### What is Docker?

Docker is a tool that allows you to bundle all the necessary parts of your system into a single container. It’s like packaging everything you need to run the system into one box. This makes sure that the system works the same way on any computer, regardless of what’s already installed on it.

### Docker Setup

We create a **Dockerfile** that contains all the steps needed to set up our system, and a **docker-compose.yml** file to manage different parts of the system (like the API and scrapers) together. This makes the deployment process much easier.

### Example Dockerfile

            FROM python:3.9-slim

            WORKDIR /app
            COPY requirements.txt .
            RUN pip install -r requirements.txt

            COPY . .

            EXPOSE 5000
            CMD \["python", "api/app.py"\]
        

The Dockerfile contains instructions for setting up the environment. It tells Docker to start with a basic Python environment, install all the necessary libraries, and then run the API.

For more information, feel free to ask questions or reach out to the development team!
